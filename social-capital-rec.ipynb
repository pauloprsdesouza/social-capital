{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from boto3.dynamodb.conditions import Key\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Constants\n",
    "TABLE_NAME = 'twitter-analytics-v2'\n",
    "RATING_FILE_PATH = 'database/ratingv2.csv'\n",
    "COLUMN_TYPES = {\n",
    "    'item_id': 'str',\n",
    "    'user_who_published': 'str',\n",
    "    'user_id': 'Int32',\n",
    "    'ranking': 'Int32',\n",
    "    'rating': 'Int32',\n",
    "    'algorithm': 'str',\n",
    "    'date': 'str'\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize resources\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_from_user(user_id):\n",
    "    \"\"\"Get tweets published by a user.\"\"\"\n",
    "    try:\n",
    "        response = table.query(\n",
    "            KeyConditionExpression=Key('PK').eq(f'Tweet#AuthorId#{user_id}'))\n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Failed to get tweets from user {user_id}: {e}\")\n",
    "    else:\n",
    "        return response['Items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(user_id, tweet_id):\n",
    "    \"\"\"Get a specific tweet published by a user.\"\"\"\n",
    "    try:\n",
    "        response = table.get_item(Key={'PK': f'Tweet#AuthorId#{user_id}', 'SK': f'TweetId#{tweet_id}'})\n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Failed to get tweet {tweet_id} from user {user_id}: {e}\")\n",
    "    else:\n",
    "        return response.get('Item', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_recommendations(file_path):\n",
    "    \"\"\"Read recommendations from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path, dtype=COLUMN_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load unique tweets from the dataset of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_data_from_proposal_method(recommendations):\n",
    "    \"\"\"Enhance recommendations with additional tweet data.\"\"\"\n",
    "    for col in ['score', 'text', 'like_count', 'retweet_count', 'reply_count', 'quote_count']:\n",
    "        if col not in recommendations.columns:\n",
    "            recommendations[col] = None \n",
    "\n",
    "    unique_items = recommendations[['item_id', 'user_who_published']].drop_duplicates()\n",
    "\n",
    "    for index, row in unique_items.iterrows():\n",
    "        tweet = get_tweet(str(row['user_who_published']), str(row['item_id']))\n",
    "\n",
    "        if tweet:\n",
    "            unique_items.at[index, 'user_id'] = recommendations.iloc[index]['user_id']\n",
    "            unique_items.at[index, 'score'] = Decimal(tweet.get('SocialCapitalScore', 0))\n",
    "            unique_items.at[index, 'text'] = tweet.get('Text', '')\n",
    "            unique_items.at[index, 'like_count'] = Decimal(tweet.get('LikeCount', 0))\n",
    "            unique_items.at[index, 'retweet_count'] = Decimal(tweet.get('RetweetCount', 0))\n",
    "            unique_items.at[index, 'reply_count'] = Decimal(tweet.get('ReplyCount', 0))\n",
    "            unique_items.at[index, 'quote_count'] = Decimal(tweet.get('QuoteCount', 0))\n",
    "    return unique_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_recommendations(recommendations, unique_items):\n",
    "    newRecommendations = pd.DataFrame()\n",
    "    for index, row in recommendations.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        who_published = str(row['user_who_published'])\n",
    "        tweet_id = str(row['item_id'])\n",
    "        algorithm = row['algorithm']\n",
    "\n",
    "        rating = recommendations[(recommendations['item_id'] == tweet_id) & (recommendations['algorithm'] == algorithm) & (recommendations['user_id'] == user_id)]['rating'].values[0]\n",
    "\n",
    "        item = unique_items[unique_items['item_id'] == tweet_id]\n",
    "        score = item['score'].values[0]\n",
    "        text = item['text'].values[0]\n",
    "        newRecommendations.at[index, 'text'] = text\n",
    "        newRecommendations.at[index, 'item_id'] = tweet_id\n",
    "        newRecommendations.at[index, 'user_who_published'] = who_published\n",
    "        newRecommendations.at[index, 'user_id'] = user_id\n",
    "        newRecommendations.at[index, 'ranking'] = 0\n",
    "        newRecommendations.at[index, 'rating'] = int(rating)\n",
    "        newRecommendations.at[index, 'score'] = score\n",
    "        newRecommendations.at[index, 'like_count'] = item['like_count'].values[0]\n",
    "        newRecommendations.at[index, 'retweet_count'] = item['retweet_count'].values[0]\n",
    "        newRecommendations.at[index, 'reply_count'] = item['reply_count'].values[0]\n",
    "        newRecommendations.at[index, 'quote_count'] = item['quote_count'].values[0]\n",
    "        newRecommendations.at[index, 'algorithm'] = f'{algorithm}-SCSA_PLUS'\n",
    "        newRecommendations.at[index, 'date'] = \"2023-08-19\"\n",
    "\n",
    "    ranking = 1\n",
    "    sortRecommendationsByUserAndAlgorithm = newRecommendations.sort_values(by=['user_id','algorithm', 'score'], ascending=False)\n",
    "\n",
    "    for index, row in sortRecommendationsByUserAndAlgorithm.iterrows():\n",
    "        sortRecommendationsByUserAndAlgorithm.at[index, 'ranking'] = ranking\n",
    "        if(ranking == 10):\n",
    "            ranking = 1\n",
    "        else: \n",
    "            ranking = ranking + 1\n",
    "\n",
    "    print(sortRecommendationsByUserAndAlgorithm.head())\n",
    "    sortRecommendationsByUserAndAlgorithm.drop('score', axis=1, inplace=True)\n",
    "\n",
    "    #return pd.concat([recommendations, sortRecommendationsByUserAndAlgorithm], ignore_index=True)\n",
    "\n",
    "    return sortRecommendationsByUserAndAlgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_recommendations_df = read_recommendations(RATING_FILE_PATH)\n",
    "logging.info(base_recommendations_df.head())\n",
    "\n",
    "proposed_recommendations = get_tweets_data_from_proposal_method(base_recommendations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = create_new_recommendations(base_recommendations_df, proposed_recommendations)\n",
    "recommendations.to_csv('output_recommendations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Processing with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)  # You can adjust the number of features\n",
    "\n",
    "# Fit and transform the 'tweet_text' column\n",
    "tfidf_matrix = tfidf.fit_transform(recommendations['text'].fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame to make it easier to work with\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Optionally, prefix the column names to distinguish text features\n",
    "tfidf_df.columns = ['text_' + str(col) for col in tfidf_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select interaction columns\n",
    "interaction_data = recommendations[['like_count', 'retweet_count', 'quote_count', 'reply_count']].fillna(0)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the interaction data\n",
    "scaled_interaction_data = scaler.fit_transform(interaction_data)\n",
    "\n",
    "# Convert the scaled data into a DataFrame\n",
    "interaction_df = pd.DataFrame(scaled_interaction_data, columns=interaction_data.columns)\n",
    "\n",
    "# Optionally, prefix the column names to distinguish interaction features\n",
    "interaction_df.columns = ['interaction_' + str(col) for col in interaction_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the TF-IDF features and the scaled interaction features\n",
    "combined_features = pd.concat([tfidf_df, interaction_df], axis=1)\n",
    "\n",
    "# If you have additional features like user or tweet IDs, you can include them as well\n",
    "combined_features['user_id'] = recommendations['user_id']\n",
    "combined_features['tweet_id'] = recommendations['item_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders\n",
    "user_id_encoder = LabelEncoder()\n",
    "tweet_id_encoder = LabelEncoder()\n",
    "algorithm_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "combined_features['user_id_encoded'] = user_id_encoder.fit_transform(combined_features['user_id'])\n",
    "combined_features['tweet_id_encoded'] = tweet_id_encoder.fit_transform(combined_features['tweet_id'])\n",
    "combined_features['algorithm_encoded'] = algorithm_encoder.fit_transform(recommendations['algorithm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target variable (assuming 'rating' is the column with your target variable)\n",
    "target = recommendations['rating'].astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrame to float32\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "\n",
    "# If using NumPy, ensure the arrays are the correct type\n",
    "y_train = np.array(y_train).astype('float32')\n",
    "\n",
    "# Replace inf with a large finite number and nan with zero\n",
    "X_train = np.nan_to_num(X_train)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "\n",
    "# Example of reshaping if your model expects one feature\n",
    "# y_train might need to be reshaped if it's a single feature\n",
    "y_train = y_train.reshape(-1, 1)  # Reshape if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Text and interaction features input\n",
    "text_interaction_input = Input(shape=(X_train.shape[1],), name=\"text_interaction_input\")\n",
    "\n",
    "# User and tweet embeddings\n",
    "user_input = Input(shape=(1,), name=\"user_input\")\n",
    "user_embedding = Embedding(input_dim=len(user_id_encoder.classes_), output_dim=50)(user_input)\n",
    "user_embedding = Flatten()(user_embedding)\n",
    "\n",
    "tweet_input = Input(shape=(1,), name=\"tweet_input\")\n",
    "tweet_embedding = Embedding(input_dim=len(tweet_id_encoder.classes_), output_dim=50)(tweet_input)\n",
    "tweet_embedding = Flatten()(tweet_embedding)\n",
    "\n",
    "# Algorithm input\n",
    "algorithm_input = Input(shape=(1,), name=\"algorithm_input\")\n",
    "algorithm_embedding = Embedding(input_dim=len(algorithm_encoder.classes_), output_dim=10)(algorithm_input)\n",
    "algorithm_embedding = Flatten()(algorithm_embedding)\n",
    "\n",
    "# Combine all inputs\n",
    "concat_layer = Concatenate()([text_interaction_input, user_embedding, tweet_embedding, algorithm_embedding])\n",
    "\n",
    "# Deep Neural Network layers\n",
    "dnn_layer = Dense(256, activation='relu')(concat_layer)\n",
    "dnn_layer = Dropout(0.2)(dnn_layer)\n",
    "tf = Dense(128, activation='relu')(dnn_layer)\n",
    "output = Dense(1)(dnn_layer)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=[text_interaction_input, user_input, tweet_input, algorithm_input], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_interaction_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_interaction_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "train_inputs = {\n",
    "    \"text_interaction_input\": X_train.drop(['user_id_encoded', 'tweet_id_encoded', 'algorithm_encoded'], axis=1),\n",
    "    \"user_input\": X_train['user_id_encoded'],\n",
    "    \"tweet_input\": X_train['tweet_id_encoded'],\n",
    "    \"algorithm_input\": X_train['algorithm_encoded']\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    \"text_interaction_input\": X_test.drop(['user_id_encoded', 'tweet_id_encoded', 'algorithm_encoded'], axis=1),\n",
    "    \"user_input\": X_test['user_id_encoded'],\n",
    "    \"tweet_input\": X_test['tweet_id_encoded'],\n",
    "    \"algorithm_input\": X_test['algorithm_encoded']\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_inputs, y_train, validation_split=0.1, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings\n",
    "predicted_ratings = model.predict(test_inputs)\n",
    "\n",
    "# Evaluate the model (you can use more sophisticated metrics as needed)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "error = mse(y_test, predicted_ratings).numpy()\n",
    "print(f\"Mean Squared Error on Test Set: {error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
