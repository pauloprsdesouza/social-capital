{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "df.head()\n",
    "\n",
    "resultsDf = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    { 'user_id':1, 'ranking': 1, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 2, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 3, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 4, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 5, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 6, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 7, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 8, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 9, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 10, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 1, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 2, 'algorithm': 'B', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 3, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 4, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 5, 'algorithm': 'B', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 6, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 7, 'algorithm': 'B', 'rating': 5 },\n",
    "    { 'user_id':1, 'ranking': 8, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 9, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':1, 'ranking': 10, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 1, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 2, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 3, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 4, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 5, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 6, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 7, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 8, 'algorithm': 'A', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 9, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 10, 'algorithm': 'A', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 1, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 2, 'algorithm': 'B', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 3, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 4, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 5, 'algorithm': 'B', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 6, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 7, 'algorithm': 'B', 'rating': 5 },\n",
    "    { 'user_id':2, 'ranking': 8, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 9, 'algorithm': 'B', 'rating': 1 },\n",
    "    { 'user_id':2, 'ranking': 10, 'algorithm': 'B', 'rating': 1 }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(data, threshold=4.0):\n",
    "    # Filter out only relevant items\n",
    "    relevant_data = data[data['rating'] >= threshold]\n",
    "    \n",
    "    # Get the rank of the first relevant item for each user\n",
    "    min_relevant_rank = relevant_data.groupby(['user_id', 'algorithm'])['ranking'].min().reset_index()\n",
    "    \n",
    "    # Compute the reciprocal rank for each user\n",
    "    min_relevant_rank['reciprocal_rank'] = 1.0 / min_relevant_rank['ranking']\n",
    "    \n",
    "    # Compute the mean of the reciprocal ranks\n",
    "    mrr = min_relevant_rank.groupby('algorithm')['reciprocal_rank'].mean()\n",
    "    \n",
    "    return mrr\n",
    "\n",
    "# Compute MRR for the data\n",
    "mrr_values = compute_mrr(df)\n",
    "resultsDf['MRR'] = mrr_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm\n",
      "B1                0.728485\n",
      "B1-SCSA_PLUS      0.777916\n",
      "CS                0.695136\n",
      "CS-SCSA_PLUS      0.669751\n",
      "SC                0.793744\n",
      "SC-SCSA_PLUS      0.786361\n",
      "SCSA              0.717515\n",
      "SCSA-SCSA_PLUS    0.735007\n",
      "Name: precision_at_k, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def compute_average_precision(dataframe, k=10, relevant_threshold=4):\n",
    "    dataframe = dataframe.sort_values(by=[\"user_id\", \"algorithm\", \"ranking\"])\n",
    "    \n",
    "    dataframe[\"is_relevant\"] = dataframe[\"rating\"] >= relevant_threshold\n",
    "\n",
    "    dataframe[\"precision_at_k\"] = dataframe.groupby([\"user_id\", \"algorithm\"])[\"is_relevant\"].cumsum() / (dataframe[\"ranking\"].astype(int))\n",
    "    \n",
    "    dataframe = dataframe[dataframe[\"ranking\"] <= k]\n",
    "\n",
    "    avg_precision = dataframe[dataframe[\"is_relevant\"] == True].groupby([\"user_id\", \"algorithm\"])[\"precision_at_k\"].mean()\n",
    "\n",
    "    return avg_precision.groupby(level=\"algorithm\").mean()\n",
    "\n",
    "map_per_algorithm = compute_average_precision(df)\n",
    "print(map_per_algorithm)\n",
    "resultsDf['MAP'] = map_per_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(df, k=10):\n",
    "    # Sort dataframe by ranking (predicted ranking)\n",
    "    df = df.sort_values(by=[\"user_id\", \"algorithm\", \"ranking\"])\n",
    "    \n",
    "    # Get top k items\n",
    "    df = df.head(k)\n",
    "    \n",
    "    # Compute the relevance of each item (1 if rating >= 4 else 0)\n",
    "    relevance = (df['rating'] >= 4).astype(int).values\n",
    "    \n",
    "    # Compute DCG\n",
    "    dcg = np.sum(relevance / np.log2(np.arange(2, k+2)))\n",
    "    \n",
    "    # Compute IDCG (ideal DCG)\n",
    "    idcg = np.sum(sorted(relevance, reverse=True) / np.log2(np.arange(2, k+2)))\n",
    "    \n",
    "    # Compute NDCG\n",
    "    return dcg / idcg if idcg != 0 else 0\n",
    "\n",
    "# Group by user and algorithm and compute NDCG@10\n",
    "ndcg_scores = df.groupby(['user_id', 'algorithm']).apply(compute_ndcg_at_k)\n",
    "\n",
    "# Average NDCG@10 for each algorithm\n",
    "avg_ndcg_scores = ndcg_scores.groupby('algorithm').mean()\n",
    "resultsDf['NDCG'] = avg_ndcg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_at_k(ratings, k):\n",
    "    relevant_items = sum(1 for rating in ratings[:k] if rating >= 4)\n",
    "    return relevant_items / k\n",
    "\n",
    "grouped = df.groupby(['user_id', 'algorithm']).apply(lambda x: x.sort_values('ranking')['rating'].tolist())\n",
    "\n",
    "# Compute Precision@10 for each group\n",
    "precision_at_10_values = grouped.apply(lambda x: compute_precision_at_k(x, 10))\n",
    "\n",
    "# Compute average Precision@10 for each algorithm\n",
    "precision_at_10_per_algorithm = precision_at_10_values.groupby('algorithm').mean()\n",
    "\n",
    "resultsDf['Precision'] = precision_at_10_per_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-value: -0.47506595255348427\n",
      "P-value: 0.6420661451287575\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "t_val, p_val = stats.ttest_ind(mrr_values, avg_ndcg_scores)\n",
    "print(\"T-value:\", t_val)\n",
    "print(\"P-value:\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test T-Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm        B1  B1-SCSA_PLUS        CS  CS-SCSA_PLUS        SC  \\\n",
      "MRR        0.748028      0.793284  0.732232      0.641063  0.792398   \n",
      "MAP        0.728485      0.777916  0.695136      0.669751  0.793744   \n",
      "NDCG       0.753666      0.788356  0.731911      0.702362  0.795544   \n",
      "\n",
      "algorithm  SC-SCSA_PLUS      SCSA  SCSA-SCSA_PLUS  \n",
      "MRR            0.778407  0.735235        0.756254  \n",
      "MAP            0.786361  0.717515        0.735007  \n",
      "NDCG           0.792450  0.738417        0.753788  \n"
     ]
    }
   ],
   "source": [
    "resultsTransposed = resultsDf.transpose()\n",
    "print(resultsTransposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7480277185501065\n"
     ]
    }
   ],
   "source": [
    "print(resultsTransposed['B1']['MRR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-value: 1.2502338252923473\n",
      "P-value: 0.25138748785670967\n",
      "T-value: -4.34531222446711\n",
      "P-value: 0.0033741069395569817\n",
      "T-value: -1.3069701975016905\n",
      "P-value: 0.2325176043356958\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "t_val, p_val = stats.ttest_rel(resultsDf.iloc[:,0], resultsDf.iloc[:,1])\n",
    "print(\"T-value:\", t_val)\n",
    "print(\"P-value:\", p_val)\n",
    "\n",
    "t_val, p_val = stats.ttest_rel(resultsDf.iloc[:,1], resultsDf.iloc[:,2])\n",
    "print(\"T-value:\", t_val)\n",
    "print(\"P-value:\", p_val)\n",
    "\n",
    "t_val, p_val = stats.ttest_rel(resultsDf.iloc[:,0], resultsDf.iloc[:,2])\n",
    "print(\"T-value:\", t_val)\n",
    "print(\"P-value:\", p_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR     0.002408\n",
      "MAP     0.002008\n",
      "NDCG    0.001101\n",
      "dtype: float64\n",
      "algorithm\n",
      "B1                0\n",
      "B1-SCSA_PLUS      0\n",
      "CS                0\n",
      "CS-SCSA_PLUS      0\n",
      "SC                0\n",
      "SC-SCSA_PLUS      0\n",
      "SCSA              0\n",
      "SCSA-SCSA_PLUS    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "variances = resultsTransposed.var(axis=1)  # Compute variance for each row (algorithm-metric combination)\n",
    "print(variances)\n",
    "\n",
    "nan_inf_counts = resultsTransposed.isin([np.nan, np.inf, -np.inf]).sum()\n",
    "print(nan_inf_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.902733929590394 0.008307888934725258\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_val = stats.ttest_rel(resultsTransposed['SCSA'],resultsTransposed['SCSA-SCSA_PLUS'])\n",
    "print(t_stat, p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Metric              Comparison   p-value  Significant\n",
      "1         MRR      B1 vs B1-SCSA_PLUS  0.010189         True\n",
      "3         MRR  SCSA vs SCSA-SCSA_PLUS  0.008308         True\n",
      "5         MAP      B1 vs B1-SCSA_PLUS  0.010189         True\n",
      "7         MAP  SCSA vs SCSA-SCSA_PLUS  0.008308         True\n",
      "9        NDCG      B1 vs B1-SCSA_PLUS  0.010189         True\n",
      "11       NDCG  SCSA vs SCSA-SCSA_PLUS  0.008308         True\n",
      "13  Precision      B1 vs B1-SCSA_PLUS  0.010189         True\n",
      "15  Precision  SCSA vs SCSA-SCSA_PLUS  0.008308         True\n"
     ]
    }
   ],
   "source": [
    "algorithms = ['SC', 'B1', 'CS', 'SCSA', 'SCSA-SCSA_PLUS', 'B1-SCSA_PLUS', 'CS-SCSA_PLUS', 'SC-SCSA_PLUS']\n",
    "metrics = ['MRR', 'MAP', 'NDCG', 'Precision']\n",
    "results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    for i in range(len(algorithms)):\n",
    "        for j in range(i+1, len(algorithms)):\n",
    "            alg1 = algorithms[i]\n",
    "            alg2 = algorithms[j]\n",
    "            # Perform t-test\n",
    "            t_stat, p_val = stats.ttest_rel(resultsTransposed[alg1],resultsTransposed[alg2])\n",
    "\n",
    "            if(alg2.startswith(f'{alg1}-')):\n",
    "                results.append({'Metric': metric, 'Comparison': f'{alg1} vs {alg2}', 'p-value': p_val})\n",
    "\n",
    "#Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#3. Adjust for multiple comparisons using the Bonferroni correction\n",
    "alpha = 0.05\n",
    "results_df['Significant'] = results_df['p-value'] < alpha\n",
    "\n",
    "print(results_df[results_df['Significant'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DV and data must be specified",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t_stat, p_val \u001b[39m=\u001b[39m pg\u001b[39m.\u001b[39;49manova(resultsTransposed)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pingouin/parametric.py:982\u001b[0m, in \u001b[0;36manova\u001b[0;34m(data, dv, between, ss_type, detailed, effsize)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[39mreturn\u001b[39;00m anovan(dv\u001b[39m=\u001b[39mdv, between\u001b[39m=\u001b[39mbetween, data\u001b[39m=\u001b[39mdata, ss_type\u001b[39m=\u001b[39mss_type, effsize\u001b[39m=\u001b[39meffsize)\n\u001b[1;32m    981\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m data \u001b[39m=\u001b[39m _check_dataframe(dv\u001b[39m=\u001b[39;49mdv, between\u001b[39m=\u001b[39;49mbetween, data\u001b[39m=\u001b[39;49mdata, effects\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbetween\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    984\u001b[0m \u001b[39m# Drop missing values\u001b[39;00m\n\u001b[1;32m    985\u001b[0m data \u001b[39m=\u001b[39m data[[dv, between]]\u001b[39m.\u001b[39mdropna()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pingouin/utils.py:365\u001b[0m, in \u001b[0;36m_check_dataframe\u001b[0;34m(data, dv, between, within, subject, effects)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39m# Check that both dv and data are provided.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m [dv, data]):\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDV and data must be specified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m \u001b[39m# Check that dv is a numeric variable\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m data[dv]\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfi\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: DV and data must be specified"
     ]
    }
   ],
   "source": [
    "t_stat, p_val = pg.anova(resultsTransposed)\n",
    "\n",
    "for metric in metrics:\n",
    "    for i in range(len(algorithms)):\n",
    "        for j in range(i+1, len(algorithms)):\n",
    "            alg1 = algorithms[i]\n",
    "            alg2 = algorithms[j]\n",
    "            # Perform t-test\n",
    "            t_stat, p_val = pg.anova(resultsTransposed[alg1],resultsTransposed[alg2])\n",
    "\n",
    "            if(alg2.startswith(f'{alg1}-')):\n",
    "                results.append({'Metric': metric, 'Comparison': f'{alg1} vs {alg2}', 'p-value': p_val})\n",
    "\n",
    "#Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#3. Adjust for multiple comparisons using the Bonferroni correction\n",
    "alpha = 0.05\n",
    "results_df['Significant'] = results_df['p-value'] < alpha\n",
    "\n",
    "print(results_df[results_df['Significant'] == True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
